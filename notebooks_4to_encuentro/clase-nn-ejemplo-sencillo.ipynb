{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tema: Redes Neuronales Feed-forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que para la notebook de la clase 5 de SVM, utilizaremos el dataset \"empty.all.csv\" que contiene como clase positiva, 900 artículos de Wikipedia en inglés que presentan la falla \"Empty Section\" y como clase negativa, contiene 900 artículos destacados. El mismo se encuentra en el subdirectorio \"miscelaneos\" del repositorio Github. Los datos se cargan como un DataFrame mediante un método de la biblioteca seaborn. A tal fin es necesario copiar el dataset en el home local de seaborn. Por defecto usa ~/seaborn-data/, en Windows: \"C:\\Users\\Nbre_Usuario\\seaborn-data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "empty = sns.load_dataset('empty.all',cache=True)\n",
    "empty.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(empty.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_empty = empty.drop('has_flaw', axis=1)\n",
    "X_empty.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_empty = empty['has_flaw']\n",
    "y_empty.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_empty, y_empty, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_test shape: {}\".format(X_test.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La celda a continuación tiene como objetivo estandarizar los valores de las características para que tengan media 0 y varianza 1. Hacemos esto pues de forma similar a cómo sucedía con SVM, las NNs son sensitivas al escalado de características. Para corroborar esto se sugiere primeramente no ejecutar la celda de estandarización y ver la performance que tiene el clasificador. Luego, ejecutar la misma y las que siguen a continuación para poder comparar la diferencia existente en la calidad predictiva del clasificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler  \n",
    "scaler = StandardScaler()  \n",
    "# fit only on training data\n",
    "scaler.fit(X_train)  \n",
    "X_train = scaler.transform(X_train)  \n",
    "# apply same transformation to test data\n",
    "X_test = scaler.transform(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción de los parámetros de la NN\n",
    "\n",
    "**activation** es la función de activación: {‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, por defecto ‘relu’.\n",
    "\n",
    "**alpha** es el parámetro de penalización del término de regularización introducido en la función a optimizar para evitar el sobre-ajuste. En la transparencia 44 de la clase 7, es referido como *lambda*.\n",
    "\n",
    "**hidden_layer_sizes** es el número de capas ocultas y de neuronas en cada capa. Por defecto, 1 capa oculta de 100 neuronas *(100,)*. Si quisiéramos poner tres capas ocultas de 10 neuronas cada una, deberíamos especificar *(10,10,10)*.\n",
    "\n",
    "**solver** es el método utilizado para realizar la optimización de pesos: {‘lbfgs’, ‘sgd’, ‘adam’}, por defecto ‘adam’.\n",
    "* *lbfgs*: optimizador de la familia de los métodos \"quasi-Newton\".\n",
    "* *sgd*: descenso del gradiente estocástico.\n",
    "* *adam*: descenso del gradiente estocástico propuesto por Kingma, Diederik, and Jimmy Ba.\n",
    "\n",
    "**learning_rate** constante positiva que nos permite moderar cuán pronunciada es la actualización de los pesos en cada paso: {‘constant’, ‘invscaling’, ‘adaptive’}, por defecto ‘constant’.\n",
    "* *constant* es un valor constante dado por el parámetro ‘learning_rate_init’.\n",
    "* *invscaling* decrementa gradualmente el learning rate en cada paso de tiempo ‘t’: effective_learning_rate = learning_rate_init / pow(t, power_t).\n",
    "* *adaptive* mantiene el learning rate constante en ‘learning_rate_init’ mientras la función de pérdida en el entrenamiento siga disminuyendo.\n",
    "\n",
    "**momentum** valor constante para actualizar el descenso del gradiente, por defecto ‘0.9’. Sólo usado cuando solver=‘sgd’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "nn = MLPClassifier(activation='logistic', solver='lbfgs', alpha=0.0001, hidden_layer_sizes=(5,), random_state=1)\n",
    "\n",
    "nn.fit(X_train, y_train)     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_model = nn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['no', 'yes']\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "mat = confusion_matrix(y_test, y_model)\n",
    "\n",
    "sns.heatmap(mat.T, square=True, annot=True, cbar=False, fmt=\"d\", xticklabels=target_names, yticklabels=target_names)\n",
    "plt.xlabel('valor verdadero')\n",
    "plt.ylabel('valor predicho');\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nngs = MLPClassifier(activation='logistic', solver='sgd', alpha=0.0001, hidden_layer_sizes=(5,), random_state=1)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'momentum': [0.7,0.8,0.9], 'learning_rate_init': [0.001,0.01,0.1]}\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(nngs, parameters, cv=5,\n",
    "                       scoring='%s_macro' % score)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    #print()\n",
    "    #print(\"Grid scores on development set:\")\n",
    "    #print()\n",
    "    #means = clf.cv_results_['mean_test_score']\n",
    "    #stds = clf.cv_results_['std_test_score']\n",
    "    #for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    #    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "    #          % (mean, std * 2, params))\n",
    "    #print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
